{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d247560d-00ff-48f9-8104-bd730c4cd09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법 (Gradient Descent)\n",
    "\n",
    "# 1. 정의 및 목적\n",
    "# 경사 하강법은 기계 학습에서 모델의 손실 함수(Loss Function) 값을 최소화하여,\n",
    "# 모델이 데이터에 가장 잘 맞도록 파라미터를 업데이트하는 반복적인 최적화 알고리즘입니다.\n",
    "# 함수의 '경사(기울기)'를 따라 낮은 쪽으로 조금씩 이동하며 최솟값을 찾아갑니다.\n",
    "\n",
    "# 2. 기본 원리\n",
    "# 산비탈에서 가장 빠르게 아래로 내려가는 경로를 찾는 것에 비유할 수 있습니다.\n",
    "# 현재 위치에서 경사가 가장 가파른 방향(기울기의 반대 방향)으로 이동해야 손실이 가장 빠르게 감소합니다.\n",
    "\n",
    "# 3. 핵심 요소\n",
    "\n",
    "# 3.1. 손실 함수 (Loss Function)\n",
    "# - 모델의 예측값과 실제 정답 사이의 차이를 측정하는 함수입니다.\n",
    "# - 경사 하강법의 목적은 이 손실 함수의 값을 최소화하는 것입니다.\n",
    "\n",
    "# 3.2. 기울기 (Gradient, ∇J)\n",
    "# - 손실 함수를 모델의 파라미터(W, b)에 대해 편미분한 값들의 벡터입니다.\n",
    "# - 현재 위치에서 손실 함수 값이 증가하는 방향과 크기를 나타냅니다.\n",
    "# - 우리는 이 기울기의 '반대 방향'으로 이동해야 합니다.\n",
    "\n",
    "# 3.3. 학습률 (Learning Rate, α)\n",
    "# - 한 번의 파라미터 업데이트 시 이동할 거리(보폭)를 결정하는 하이퍼파라미터입니다.\n",
    "# - **α가 너무 크면**: 최솟값을 지나쳐 발산하거나 불안정하게 진동할 수 있습니다.\n",
    "# - **α가 너무 작으면**: 최솟값에 도달하는 데 시간이 너무 오래 걸릴 수 있습니다.\n",
    "\n",
    "# 4. 파라미터 업데이트 공식\n",
    "# 경사 하강법은 아래의 수식에 따라 모델의 파라미터(W, b)를 반복적으로 업데이트합니다.\n",
    "\n",
    "# 현재 파라미터 (New_W) = 현재 파라미터 (Old_W) - (학습률 α) * (손실 함수의 기울기 ∇J)\n",
    "\n",
    "# **W_new = W_old - α * ∇J(W_old)**\n",
    "\n",
    "# 5. 경사 하강법의 종류 (데이터 사용 방식에 따른 분류)\n",
    "\n",
    "# 5.1. 배치 경사 하강법 (Batch Gradient Descent)\n",
    "# - 전체 훈련 데이터셋을 사용하여 기울기를 계산하고 파라미터를 한 번 업데이트합니다.\n",
    "# - 장점: 안정적인 수렴을 보장합니다.\n",
    "# - 단점: 데이터셋이 클 경우 계산 비용과 시간이 매우 많이 소요됩니다.\n",
    "\n",
    "# 5.2. 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)\n",
    "# - 전체 데이터셋이 아닌, **랜덤하게 선택된 하나의 데이터 샘플**에 대해서만 기울기를 계산하고 업데이트합니다.\n",
    "# - 장점: 계산 속도가 빠르고, 배치 GD보다 지역 최솟값(Local Minima)에 덜 갇힙니다.\n",
    "# - 단점: 손실 함수 그래프에서 이동 경로가 불안정하고 노이즈가 심합니다.\n",
    "\n",
    "# 5.3. 미니 배치 경사 하강법 (Mini-batch Gradient Descent)\n",
    "# - 전체 데이터셋에서 무작위로 추출한 **작은 배치(Mini-batch)** 크기의 데이터를 사용하여 기울기를 계산하고 업데이트합니다. (예: 32개, 64개 샘플)\n",
    "# - 장점: 배치 GD의 안정성과 SGD의 속도를 결합한 가장 일반적인 방법입니다.\n",
    "\n",
    "# 6. 경사 하강법의 한계 및 발전\n",
    "# - 기본적인 경사 하강법은 학습률 설정에 민감하고, '지역 최솟값'에 갇힐 위험이 있으며,\n",
    "# - 기울기가 너무 작은 '안장점(Saddle Point)'에서 학습이 느려지는 문제가 있습니다.\n",
    "# - 이를 개선하기 위해 **모멘텀(Momentum)**, **AdaGrad**, **RMSProp**, 그리고 가장 널리 사용되는 **Adam** 등의 **고급 최적화 알고리즘**이 개발되었습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
